---
title: 'Fase 2: Clustering'
author: "Irving, Chuy"
date: "2025-02-10"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(kableExtra)
library(tidyr)
library(ggplot2)

library(lubridate)
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
```

```{r, include=FALSE}
movies <- read.csv("./movies.csv")

```

## 1.1 Seleccion de columnas
```{r echo=FALSE}
columns <- c("id", "popularity", "budget", "revenue", "runtime", "genresAmount", "productionCountriesAmount", "releaseDate", "voteCount", "voteAvg", "actorsPopularity", "actorsAmount", "castWomenAmount", "castMenAmount")

selected_movies_clustr <- movies[,columns]

selected_movies_clustr

```

# Conversion de columnas no numericas
```{r echo=FALSE}
pupoular_sum <- selected_movies_clustr %>%
  separate_rows( actorsPopularity, sep = "|") %>%
  mutate(actorsPopularity = as.numeric(actorsPopularity)) %>%
  group_by(id) %>%
  summarise(totalActorPopularity = sum(actorsPopularity, na.rm = TRUE))

pupoular_sum

df_final <- selected_movies_clustr %>%
  left_join(pupoular_sum, by = "id")

df_final <- df_final %>% select(-actorsPopularity)

df_final$releaseDate <- as.Date(df_final$releaseDate, format = "%d/%m/%Y")

# df_final$year <- year(df_final$releaseDate)
# df_final$month <- month(df_final$releaseDate)
# df_final$day <- day(df_final$releaseDate)

df_final <- df_final %>% select(-releaseDate)
df_ids <- selected_movies_clustr %>% select(id)

df_final <- df_final %>% select(-id)


# columnas que se deben pasar a char

# data filtering
df_final <- df_final %>%
  mutate(
    castWomenAmount = ifelse(grepl("^[0-9]+$", castWomenAmount), as.integer(castWomenAmount), NA),
    castMenAmount = ifelse(grepl("^[0-9]+$", castMenAmount), as.integer(castMenAmount), NA)
  )

# Replace remaining NA values with 0
df_final$castWomenAmount[is.na(df_final$castWomenAmount)] <- 0
df_final$castMenAmount[is.na(df_final$castMenAmount)] <- 0

# Aqui se puede probar con tamanios distintos de muestras. 
# df_final <- df_final[sample(nrow(df_final), 100), ]

z_scores <- scale(df_final) 
outliers <- apply(abs(z_scores) > 3, 2, sum)  # Count extreme values
print(outliers)

df_final <- df_final %>%
  mutate(across(where(is.numeric), ~ ifelse(. > quantile(., 0.99), quantile(., 0.99), .)))

df_final

```
## Escalado de los datos
Es importante escalar los datos antes de realizar agrupamientos cuando se utilizan tecnicas de aprendizaje no supervisado como lo es el clustering. Dentro del data set tenemos unicamente vlores numericos, entonces podemos escalarlos para que el agrupamiento funcione mejor.

```{r echo=FALSE}
# Comentado de momento, escalar los datos impide que se vea el grafico de codo bien
df_final <- as.data.frame(scale(df_final))  # Convert back to data.frame
# colSums(is.na(df_final))
```

## Estadistico de Hopkins 

Es importante saber si es plausible hacer un agrupamiento de datos con la data frame que tenemos. Para ello hemos decidido utilizar el estadistico de hopkins
```{r echo=FALSE}
set.seed(123)
hopkins(df_final)
```
Como se puede observar el data set tiene bastante potencial para hacer grupos.Es raro que los datos esten tan bien relacionados (el valor fue de 1 en algunas pruebas) pero si probamos revolviendo los datos de igual manera se obtiene un numero cercano a 1. Lo cual implica que los datos pueden agruparse bastante bien

```{r echo=FALSE}
set.seed(123)
df_shuffled <- df_final[sample(nrow(df_final)), ]
hopkins(df_shuffled)
```
Con esto practicamente nos aseguramos de que nuestros datos pueden ser agrupados exitosamente. Antes de proceder con la definicion de grupos, haremos un un VAT (Evaluacion visual de tendencia por sus siglas en ingles). Este metodo grafico ayudara a corroborar el estdistico de hopkins de manera visual. Si se ve un patron visible y no aleatorio eso quiere decir que nuevamente los datos son adecuados para agruparse. Nota: Tomamos una muestra aleatorio del data set con el fin de ahorrar recursos computacionales. EL dataset tiene 10000 lineas, por lo que tomar el data set completo tomaria demasiado tiempo

```{r echo=FALSE}
set.seed(123)
sampled_df <- df_final[sample(nrow(df_final), 1000), ]  # muestra
dist_matrix <- dist(sampled_df)
fviz_dist(dist_matrix, show_labels = FALSE)
```

Puede observarse un patron visible, por lo que podemos decir que en efecto la data que tenemos puede ser agrupada. Puede que se vea un poco opaco, pero esto ocurre debido a que realmente no estamos usando la totalidad de los datos. Ahora que ya hemos confirmado que podemos agrupar, hay que definir la cantidad de grupos que deseamos hacer mediante el clustering. El numero de grupos lo obtendremos mediante el grafico de codo.

## Grafico de codo
``` {r echo=FALSE}
wss=0

wss <- sapply(1:8, function(i) sum(kmeans(df_final, centers = i)$withinss))

# Graficando el codo
plot(1:8, wss, type = "b", pch = 19, col = "blue", 
     xlab = "Numero de Clusters", ylab = "Suma de cuadrados dentro de los grupos",
     main = "Metodo del Codo")

sil_width <- sapply(2:8, function(i) {
  km <- kmeans(df_final, centers = i)
  ss <- silhouette(km$cluster, dist(df_final))
  mean(ss[, 3])
})

plot(2:8, sil_width, type = "b", pch = 19, col = "blue", 
     xlab = "Número de Clusters", ylab = "Ancho de Silueta",
     main = "Método de la Silueta")
```

Como se puede observar el grafico de codo indica que la cantidad ideal de grupos es 3 o 4. Justo en esa parte de la grafica es donde se ve la forma que indica la cantidad de grupos apropiados para realizar el clustering. 

## Agrupamiento

Ahora que ya conocemos el numero de grupos es necesario agrupar los datos utilizando algoritmos. Se usaran y evaluaron 2 principalmente "Kmeans" y "Clustering jerarquico" con la cantidad de 3 grupos.


### Kmeans
```{r echo=FALSE}
set.seed(123) 
km <- kmeans(df_final,3,iter.max = 1000)

df_final$grupo <- km$cluster

km

fviz_cluster(km, data = df_final)

km$withinss
```

### Clustering jerarquico
```{r echo=FALSE}

df_dist <- dist(df_final)
hc<-hclust(df_dist, method = "ward.D2") #Genera el clustering jerarquico
plot(hc, cex=0.5, axes=FALSE) #Genera el dendograma
rect.hclust(hc,k=3)

```

```{r echo=FALSE}
groups<-cutree(hc,k=3) 
df_final$gruposHC<-groups

table(df_final$gruposHC) # tamanio de los grupos
by(df_final, df_final[,"gruposHC"], colMeans)

```

## Evaluacion con metodo de silhueta de K-means
```{r echo=FALSE}

silkm <- silhouette(km$cluster,df_dist)
silkm
mean(silkm[,3])

fviz_silhouette(silkm, main = "Silhueta para K-Medias", color = "cluster")

```

## Evaluacion con metodo de silhueta para Clustering Jerarquico
```{r echo=FALSE}

silhc<-silhouette(groups,df_dist)
mean(silhc[,3]) #

fviz_silhouette(silhc, main = "Silhueta para clustering jerarquico", color = "cluster")

```

Al final se puede observar que el algoritmo de Kmedias fue ligeramente mejor que el clustering jerarquico. Por lo que se utilizaran los grupos creados mediante dicho algoritmo. Aunque la metrica de la silhueta muestra un valor que no es tan cercano a 1, de igual manera los grupos son evidentes de manera grafica. Ahora se pueden analizar los grupos

```{r echo=FALSE}


df_final <- cbind(df_ids, df_final)

df_final <- df_final %>%
  left_join(movies %>% select(id, genres, productionCompany, productionCountry, productionCompanyCountry, director, actors, originalLanguage), by = "id")

df_final

df_final %>%
  group_by(grupo) %>%
  summarise(across(where(is.numeric), list(media = mean, mediana = median, sd = sd), na.rm = TRUE))

df_final %>%
  separate_rows(genres, sep = "\\|") %>%
  count(grupo, genres) %>%
  arrange(grupo, desc(n))

df_final %>%
  separate_rows(productionCompany, sep = "\\|") %>%
  count(grupo, productionCompany) %>%
  arrange(grupo, desc(n))


```