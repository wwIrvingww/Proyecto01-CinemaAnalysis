---
title: 'Fase 2: Clustering'
author: "Irving, Chuy"
date: "2025-02-10"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(kableExtra)
library(tidyr)
library(ggplot2)

library(lubridate)
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el numero de clusters optimo
library(factoextra) #Para hacer graficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
```

```{r, include=FALSE}
movies <- read.csv("./movies.csv")

```

## 1.1 Seleccion de columnas

Estaremos eligiendo los valores numericos de las siguientes columnas: "id", "popularity", "budget", "revenue", "runtime", "genresAmount", "productionCountriesAmount", "releaseDate", "voteCount", "voteAvg", "actorsPopularity", "actorsAmount", "castWomenAmount", "castMenAmount"

```{r echo=FALSE}
columns <- c("id", "popularity", "budget", "revenue", "runtime", "genresAmount", "productionCountriesAmount", "releaseDate", "voteCount", "voteAvg", "actorsPopularity", "actorsAmount", "castWomenAmount", "castMenAmount")

selected_movies_clustr <- movies[,columns]

head(selected_movies_clustr)

```

# Conversion de columnas no numericas
Para todos los datos nos aseguraremos de que sean valores numericos. Ademas en el caso de actorsPopularity, parseamos la popularidad de todos usando los pipes "|" y la suamos para tener una sola columna.
```{r echo=FALSE}
pupoular_sum <- selected_movies_clustr %>%
  separate_rows( actorsPopularity, sep = "|") %>%
  mutate(actorsPopularity = as.numeric(actorsPopularity)) %>%
  group_by(id) %>%
  summarise(totalActorPopularity = sum(actorsPopularity, na.rm = TRUE))

head(pupoular_sum)

df_final <- selected_movies_clustr %>%
  left_join(pupoular_sum, by = "id")

df_final <- df_final %>% select(-actorsPopularity)

df_final$releaseDate <- as.Date(df_final$releaseDate, format = "%d/%m/%Y")

# df_final$year <- year(df_final$releaseDate)
# df_final$month <- month(df_final$releaseDate)
# df_final$day <- day(df_final$releaseDate)

df_final <- df_final %>% select(-releaseDate)
df_ids <- selected_movies_clustr %>% select(id)

df_final <- df_final %>% select(-id)


# columnas que se deben pasar a char

# data filtering
df_final <- df_final %>%
  mutate(
    castWomenAmount = ifelse(grepl("^[0-9]+$", castWomenAmount), as.integer(castWomenAmount), NA),
    castMenAmount = ifelse(grepl("^[0-9]+$", castMenAmount), as.integer(castMenAmount), NA)
  )

# Replace remaining NA values with 0
df_final$castWomenAmount[is.na(df_final$castWomenAmount)] <- 0
df_final$castMenAmount[is.na(df_final$castMenAmount)] <- 0

# Aqui se puede probar con tamanios distintos de muestras. 
# df_final <- df_final[sample(nrow(df_final), 100), ]
```

Aqui lo que se hace es contaibilizar los puntos atipicos. Al realizar bastantes pruebas nos dimos cuenta que estos valores hacen mucho ruido y generan problemas al momento de realizar los grupos. Entonces vemos si hay bastantes  atipicos y luego los arreglamos para luego hacer el clustering.
```{r echo=FALSE}
z_scores <- scale(df_final) 
outliers <- apply(abs(z_scores) > 3, 2, sum)  # Count extreme values
print(outliers)

df_final <- df_final %>%
  mutate(across(where(is.numeric), ~ ifelse(. > quantile(., 0.99), quantile(., 0.99), .)))

head(df_final)

```


```{r echo=FALSE}
# Comentado de momento, escalar los datos impide que se vea el grafico de codo bien
# df_final <- as.data.frame(scale(df_final))  # Convert back to data.frame
# colSums(is.na(df_final))
```

## Estadistico de Hopkins 

Es importante saber si es plausible hacer un agrupamiento de datos con la data frame que tenemos. Para ello hemos decidido utilizar el estadistico de hopkins
```{r echo=FALSE}
set.seed(123)
hopkins(df_final)
```
Como se puede observar el data set tiene bastante potencial para hacer grupos.Es raro que los datos esten tan bien relacionados (el valor fue de 1 en algunas pruebas) pero si probamos revolviendo los datos de igual manera se obtiene un numero cercano a 1. Lo cual implica que los datos pueden agruparse bastante bien

```{r echo=FALSE}
set.seed(123)
df_shuffled <- df_final[sample(nrow(df_final)), ]
hopkins(df_shuffled)
```
Con esto practicamente nos aseguramos de que nuestros datos pueden ser agrupados exitosamente. Antes de proceder con la definicion de grupos, haremos un un VAT (Evaluacion visual de tendencia por sus siglas en ingles). Este metodo grafico ayudara a corroborar el estdistico de hopkins de manera visual. Si se ve un patron visible y no aleatorio eso quiere decir que nuevamente los datos son adecuados para agruparse. Nota: Tomamos una muestra aleatorio del data set con el fin de ahorrar recursos computacionales. EL dataset tiene 10000 lineas, por lo que tomar el data set completo tomaria demasiado tiempo

```{r echo=FALSE}
set.seed(123)
sampled_df <- df_final[sample(nrow(df_final), 1000), ]  # muestra
dist_matrix <- dist(sampled_df)
fviz_dist(dist_matrix, show_labels = FALSE)
```

Puede observarse un patron visible, por lo que podemos decir que en efecto la data que tenemos puede ser agrupada. Puede que se vea un poco opaco, pero esto ocurre debido a que realmente no estamos usando la totalidad de los datos. Ahora que ya hemos confirmado que podemos agrupar, hay que definir la cantidad de grupos que deseamos hacer mediante el clustering. El numero de grupos lo obtendremos mediante el grafico de codo.

## Grafico de codo
``` {r echo=FALSE}
wss=0

wss <- sapply(1:8, function(i) sum(kmeans(df_final, centers = i)$withinss))

# Graficando el codo
plot(1:8, wss, type = "b", pch = 19, col = "blue", 
     xlab = "Numero de Clusters", ylab = "Suma de cuadrados dentro de los grupos",
     main = "Metodo del Codo")

#sil_width <- sapply(2:8, function(i) {
#  km <- kmeans(df_final, centers = i)
#  ss <- silhouette(km$cluster, dist(df_final))
#  mean(ss[, 3])
#})

#plot(2:8, sil_width, type = "b", pch = 19, col = "blue", 
#     xlab = "Numero de Clusters", ylab = "Ancho de Silueta",
#     main = "Metodo de la Silueta")
```

Como se puede observar el grafico de codo indica que la cantidad ideal de grupos es 3 o 4. Justo en esa parte de la grafica es donde se ve la forma que indica la cantidad de grupos apropiados para realizar el clustering. 

## Agrupamiento

Ahora que ya conocemos el numero de grupos es necesario agrupar los datos utilizando algoritmos. Se usaran y evaluaron 2 principalmente "Kmeans" y "Clustering jerarquico" con la cantidad de 3 grupos.


### Kmeans
```{r echo=FALSE}
set.seed(123) 
km <- kmeans(df_final,3,iter.max = 1000)

df_final$grupo <- km$cluster

head(km)

fviz_cluster(km, data = df_final, ellipse.type = "norm")

km$withinss

```

Aqui podemos ver el valor de cuanto varian los grupos resultantes del kmea
```{r echo=FALSE}
km$withinss

```

### Clustering jerarquico
```{r echo=FALSE}

df_dist <- dist(df_final)
hc<-hclust(df_dist, method = "ward.D2") #Genera el clustering jerarquico
plot(hc, cex=0.5, axes=FALSE) #Genera el dendograma
rect.hclust(hc,k=3)

```

```{r echo=FALSE}
groups<-cutree(hc,k=3) 
df_final$gruposHC<-groups

table(df_final$gruposHC) # tamanio de los grupos
by(df_final, df_final[,"gruposHC"], colMeans)

```

## Evaluacion con metodo de silhueta de K-means
```{r echo=FALSE}

silkm <- silhouette(km$cluster,df_dist)

mean(silkm[,3])

fviz_silhouette(silkm, main = "Silhueta para K-Medias", color = "cluster")

```

## Evaluacion con metodo de silhueta para Clustering Jerarquico
```{r echo=FALSE}

silhc<-silhouette(groups,df_dist)
mean(silhc[,3]) #

fviz_silhouette(silhc, main = "Silhueta para clustering jerarquico", color = "cluster")

```

Al final se puede observar que el algoritmo de Kmedias fue ligeramente mejor que el clustering jerarquico. Por lo que se utilizaran los grupos creados mediante dicho algoritmo. Aunque la metrica de la silhueta muestra un valor que no es tan cercano a 1, de igual manera los grupos son evidentes de manera grafica. Ahora se pueden analizar los grupos

```{r echo=FALSE}

df_final <- cbind(df_ids, df_final)
df_final <- df_final %>%
  left_join(movies %>% select(id, genres, productionCompany, productionCountry, productionCompanyCountry, director, actors, originalLanguage), by = "id")

```

# Analisis de las variables numericas

En la siguiente parte se muestran graficas que muestran la medida de tendencia central de las varaibles nmumericas. Ademas de eso, colocamos graficos de barras sobre las variables numericas agrupando por los clusters. Esto con el fin de encontrar patrones, relaciones y comportamientos interesantes entre los datos agrupados.

```{r echo=FALSE}

df_final %>%
  group_by(grupo) %>%
  summarise(across(where(is.numeric), list(media = mean, mediana = median, sd = sd), na.rm = TRUE))

df_final %>%
  filter(grupo == 1) %>%
  pivot_longer(cols = c(budget, revenue, runtime, genresAmount, voteCount, voteAvg, actorsAmount, totalActorPopularity), 
               names_to = "variable", values_to = "valor") %>%
  ggplot(aes(x = valor, fill = as.factor(grupo))) +
  geom_histogram(alpha = 0.6, bins = 30, position = "identity") +
  facet_wrap(~ variable, scales = "free") +
  labs(title = "Distribucion de Variables Numericas para Grupo 1", x = "Valor", y = "Frecuencia") +
  theme_minimal()

# Histograma para los demas grupos
df_final %>%
  filter(grupo != 1) %>%
  pivot_longer(cols = c(budget, revenue, runtime, genresAmount, voteCount, voteAvg, actorsAmount, totalActorPopularity), 
               names_to = "variable", values_to = "valor") %>%
  ggplot(aes(x = valor, fill = as.factor(grupo))) +
  geom_histogram(alpha = 0.6, bins = 30, position = "identity") +
  facet_wrap(~ variable, scales = "free") +
  labs(title = "Distribucion de Variables Numericas para Otros Grupos", x = "Valor", y = "Frecuencia") +
  theme_minimal()

df_promedios <- df_final %>%
  group_by(grupo) %>%
  summarise(across(c(budget, revenue, runtime, genresAmount, voteCount, voteAvg, actorsAmount, totalActorPopularity), 
                   mean, na.rm = TRUE), .groups = "drop") %>%
  pivot_longer(cols = -grupo, names_to = "variable", values_to = "promedio")

# Variables con valores muy grandes
df_promedios_grandes <- df_promedios %>%
  filter(variable %in% c("budget", "revenue"))

ggplot(df_promedios_grandes, aes(x = as.factor(grupo), y = promedio, fill = as.factor(grupo))) +
  geom_col(position = "dodge") +
  facet_wrap(~ variable, scales = "free") +
  labs(title = "Promedio de budget y revenue por grupo", x = "Grupo", y = "Promedio") +
  theme_minimal()

# Variables con valores mas peque√±os
df_promedios_pequenos <- df_promedios %>%
  filter(variable %in% c("runtime", "genresAmount", "voteCount", "voteAvg", "actorsAmount", "totalActorPopularity"))

ggplot(df_promedios_pequenos, aes(x = as.factor(grupo), y = promedio, fill = as.factor(grupo))) +
  geom_col(position = "dodge") +
  facet_wrap(~ variable, scales = "free") +
  labs(title = "Promedio de Variables numericas", x = "Grupo", y = "Promedio") +
  theme_minimal()



```

```{r echo=FALSE}

ggplot(df_final, aes(x = as.factor(grupo), y = budget)) +
  geom_boxplot() +
  labs(title = "Distribucion del Presupuesto por Grupo", x = "Grupo", y = "Presupuesto")

```

# Analisis de frecuencia con variables categoricas

Ahora en esta aparte realizamos la misma estrategia. Tomamos los datos categoricos y obtenemos las frecuencias de forma agrupada. De esta manera conocemos las tendencias de los clusters encontrados.
```{r  echo=FALSE}
df_final %>%
  separate_rows(genres, sep = "\\|") %>%
  count(grupo, genres) %>%
  ggplot(aes(x = genres, y = n, fill = as.factor(grupo))) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Frecuencia de Generos por Grupo", x = "Genero", y = "Frecuencia")

df_final <- df_final %>%
  mutate(across(where(is.character), ~ iconv(.x, from = "latin1", to = "UTF-8")))

df_final %>%
  separate_rows(productionCompany, sep = "\\|") %>%
  count(grupo, productionCompany) %>%
  group_by(grupo) %>%
  slice_max(n, n = 10) %>%  # Filtrar los 10 mas frecuentes de cada grupo
  ungroup() %>%
  ggplot(aes(x = reorder(productionCompany, n), y = n, fill = as.factor(grupo))) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Top 10 Production Companies por Grupo", x = "Production Company", y = "Frecuencia") +
  theme_minimal()

df_final %>%
  separate_rows(productionCountry, sep = "\\|") %>%
  count(grupo, productionCountry) %>%
  group_by(grupo) %>%
  slice_max(n, n = 10) %>%  # Filtrar los 10 paises mas frecuentes por grupo
  ungroup() %>%
  ggplot(aes(x = reorder(productionCountry, n), y = n, fill = as.factor(grupo))) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Top 10 Production Countries por Grupo", x = "Pais", y = "Frecuencia") +
  theme_minimal()

df_final %>%
  separate_rows(director, sep = "\\|") %>%
  count(grupo, director) %>%
  group_by(grupo) %>%
  slice_max(n, n = 10) %>%  # Filtrar los 10 paises mas frecuentes por grupo
  ungroup() %>%
  ggplot(aes(x = reorder(director, n), y = n, fill = as.factor(grupo))) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Top 10 Directores por Grupo", x = "Director", y = "Frecuencia") +
  theme_minimal()

```

# Conclusiones 

Luego de ver los resultados del agrupamiento tenemos 2 grupos que llaman la atencion principalmente, el grupo 1 y el grupo 2. Analizando primero el grupo 2 nos podemos dar cuenta que es el grupo que tiene mas datos de todos. Este es un cluster bastante grande y que tiene una gran variedad de peliculas. Este es particularmente para util ver cuales son los datos mas comunes y populares dentro del set de datos en general. En este se pueden ver varios datos interesantes como los generos de peliculas mas comunes, por ejemplo, Drama, accion, thriller, comedia y horrar, son de los datos que mas se repiten en el grupo 1. Osea que en general son los generos de pelicula mas producidos. Ademas de esto podemos ver a las productoras, siendo las mas recurrentes Warner Bros y Universal. Otra cosa es que en general casi todas las peliculas fueron producidas en algun punto en Estados unidos. Luego se puede ver que hay Directores que han participado en una cantidad enorme de proyectos en comparacion a los demas. Siendo Sam Liu el mayor participe en la direccion de peliculas, seguido de Kunihiko Yuyama y Woody Allen.  

Pero es importante mencionar algo, que sea el cluster de peliculas mas grande no quiere decir que sea el mas exitoso. La otra parte interesante de este analisis se encuentra en el grupo 1. Como se pudo observar en las graficas, el grupo 4 tiene menor representacion y frecuencia en casi todas las ocasiones, pero esto no es algo malo. Si observamos la grafica de valores promedio por grupo, podemos ver que el budget y revenue de las peliculas del grupo 2 no es muy alto en comparacion a los demas grupos. Las peliculas del grupo 2, son de bajo presupuesto y por eso es que son las mas comunes dentro de todos. Mientras que las peliculas del grupo 1, son los que fueron agrupados por ser mas costosos, pero con mucho mas ganancia en retorno.  Las peliculas con la mayor cantidad de budget por lo general tienen mayor popularidad en los actores, mejor nota promedio y son peliculas mas largas. Y finalmente una relacion bastante interesante que encontramos es la relacion del director con las peliculas. Al ver el top 10 directores recurrentes en las peliculas (por grupo) podemos destacar los nombres de muchos directores de cine famosos que han hecho grandes exitos. Directores como Steven Spilberg, Peter Jackson, Michael Bay, George Lucas, Christopher Nolan, entro otros. 

Al final los grupos dan a entender que estan categorizados respecto al costo, el revenue y el exito que estas peliculas han tenido debido a sus actores y/o directores. 